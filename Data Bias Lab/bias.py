# -*- coding: utf-8 -*-
"""Bias.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sq4sMB22lU6jI8KalTMMUXYPgDJjvKLV
"""

import pandas as pd
from bs4 import BeautifulSoup
import re

html_path = "/content/trimet_stopevents_2022-12-07.html"
with open(html_path, "r", encoding="utf-8") as file:
    html_content = file.read()

soup = BeautifulSoup(html_content, "html.parser")

output = {"trips": []}
filtered_cols = ["vehicle_number", "arrive_time", "location_id", "ons", "offs"]

for h2 in soup.find_all("h2"):
    h2_text = h2.get_text().strip()
    match = re.search(r"PDX_TRIP\s*(\d+)", h2_text)
    if match:
        trip_id = match.group(1)
    else:
        pass
        continue

    table = h2.find_next_sibling("table")
    if not table:
        pass
        continue

    header_row = table.find("thead").find("tr") if table.find("thead") else table.find("tr")
    if not header_row:
        pass
        continue

    headers = [th.get_text().strip() for th in header_row.find_all("th")]

    column_indices = {}
    try:
        for col in filtered_cols:
            column_indices[col] = headers.index(col)
    except ValueError:
        pass
        continue

    rows = table.find_all("tr")[1:]
    table_data = []

    for row in rows:
        cells = row.find_all("td")
        if len(cells) < len(headers):
            continue

        row_data = {}
        for col, idx in column_indices.items():
            if idx < len(cells):
                row_data[col] = cells[idx].get_text().strip()

        if row_data:
            table_data.append(row_data)

    output["trips"].append({
        "trip_id": trip_id,
        "table_data": table_data
    })

all_rows = []

for trip in output["trips"]:
    trip_id = trip["trip_id"]
    for row in trip["table_data"]:
        row_with_id = row.copy()
        row_with_id["trip_id"] = trip_id
        all_rows.append(row_with_id)

stops_df = pd.DataFrame(all_rows)

print(stops_df.head())

import datetime
base_date = datetime.date(2022, 12, 7)

def seconds_to_timestamp(seconds):
    seconds = int(seconds)
    return datetime.datetime.combine(base_date, datetime.time()) + datetime.timedelta(seconds=seconds)

stops_df['tstamp'] = stops_df['arrive_time'].apply(seconds_to_timestamp)

print(stops_df.head())
print(stops_df.count())

print(stops_df['vehicle_number'].nunique())
print(stops_df['location_id'].nunique())

print(stops_df['tstamp'].min())
print(stops_df['tstamp'].max())

print(stops_df[stops_df['ons'].astype(int) >= 1].count())

mod_df = stops_df[stops_df['location_id'].astype(int)  == 6913]
print(mod_df.count())
print(mod_df['vehicle_number'].nunique())
print(mod_df[mod_df['ons'].astype(int) >= 1].count())

mod_df = stops_df[stops_df['location_id'].astype(int)  == 4062]
print(mod_df.count())
total_ons = mod_df['ons'].astype(int).sum()
print(total_ons)
total_offs = mod_df['offs'].astype(int).sum()
print(total_offs)
print(mod_df[mod_df['ons'].astype(int) >= 1].count())

from scipy.stats import binomtest

stop_events_per_bus = stops_df.groupby('vehicle_number').size()

stop_events_with_boardings = stops_df[stops_df['ons'].astype(int) >= 1].groupby('vehicle_number').size()

boarding_counts = stop_events_with_boardings.reindex(stop_events_per_bus.index, fill_value=0)
percent_with_boardings = (boarding_counts / stop_events_per_bus) * 100


total_stops = len(stops_df)
total_boarding_stops = (stops_df['ons'].astype(int) >= 1).sum()
overall_p = total_boarding_stops / total_stops

p_values = {}
for bus in stop_events_per_bus.index:
    n = stop_events_per_bus[bus]
    k = boarding_counts[bus]
    test = binomtest(k, n, overall_p, alternative='two-sided')
    p_values[bus] = test.pvalue

p_values_series = pd.Series(p_values)


result_df = pd.DataFrame({
    'total_stops': stop_events_per_bus,
    'stops_with_boardings': boarding_counts,
    'percent_boardings': percent_with_boardings,
    'binomial_p_value': p_values_series
})

alpha = 0.05

significant_vehicles = result_df[result_df['binomial_p_value'] < alpha]

print(significant_vehicles[['binomial_p_value']])

df_relpos = pd.read_csv("/content/trimet_relpos_2022-12-07.csv")

df_relpos.head()

from scipy.stats import ttest_ind

all_relpos_values = df_relpos['RELPOS'].values

p_values = {}

for vehicle_id, group in df_relpos.groupby('VEHICLE_NUMBER'):
    vehicle_relpos = group['RELPOS'].values
    if len(vehicle_relpos) >= 2:
        t_result = ttest_ind(vehicle_relpos, all_relpos_values, equal_var=False)
        p_values[vehicle_id] = t_result.pvalue
    else:
        p_values[vehicle_id] = None

relpos_pvalues_df = pd.DataFrame.from_dict(p_values, orient='index', columns=['p_value'])

significant = relpos_pvalues_df[relpos_pvalues_df['p_value'] < 0.05]

print(significant)

total_ons = stops_df['ons'].astype(int).sum()
print(total_ons)
total_offs = stops_df['offs'].astype(int).sum()
print(total_offs)

from scipy.stats import chi2_contingency

stops_df['ons'] = pd.to_numeric(stops_df['ons'], errors='coerce')
stops_df['offs'] = pd.to_numeric(stops_df['offs'], errors='coerce')
df_clean = stops_df.dropna(subset=['ons', 'offs'])

ons_per_vehicle = df_clean.groupby('vehicle_number')['ons'].sum()
offs_per_vehicle = df_clean.groupby('vehicle_number')['offs'].sum()

total_ons = ons_per_vehicle.sum()
total_offs = offs_per_vehicle.sum()

p_values = {}

for vehicle in ons_per_vehicle.index:
    vehicle_ons = ons_per_vehicle[vehicle]
    vehicle_offs = offs_per_vehicle[vehicle]

    if vehicle_ons + vehicle_offs == 0:
        continue

    other_ons = total_ons - vehicle_ons
    other_offs = total_offs - vehicle_offs

    contingency = [[vehicle_ons, vehicle_offs], [other_ons, other_offs]]

    chi2, p, dof, expected = chi2_contingency(contingency)
    p_values[vehicle] = p

pval_df = pd.DataFrame.from_dict(p_values, orient='index', columns=['p_value'])
significant = pval_df[pval_df['p_value'] < 0.05]
print(significant.sort_values('p_value'))